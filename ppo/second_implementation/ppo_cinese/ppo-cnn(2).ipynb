{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install setuptools wheel swig\n!pip install gymnasium==1.1 gymnasium[box2d] torch numpy matplotlib moviepy -q\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:05:18.860590Z","iopub.execute_input":"2025-07-07T08:05:18.861102Z","iopub.status.idle":"2025-07-07T08:07:24.562010Z","shell.execute_reply.started":"2025-07-07T08:05:18.861079Z","shell.execute_reply":"2025-07-07T08:07:24.561226Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.2.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\nCollecting swig\n  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\nDownloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: swig\nSuccessfully installed swig-4.3.1\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.5/965.5 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.16.11 requires gymnasium==0.29.0, but you have gymnasium 1.1.0 which is incompatible.\nstable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!mkdir /kaggle/working/saved_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:18:26.224025Z","iopub.execute_input":"2025-07-07T08:18:26.224324Z","iopub.status.idle":"2025-07-07T08:18:26.387705Z","shell.execute_reply.started":"2025-07-07T08:18:26.224304Z","shell.execute_reply":"2025-07-07T08:18:26.386785Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\nimport gymnasium as gym\nfrom gymnasium.wrappers import GrayscaleObservation, ResizeObservation\nimport numpy as np\nimport os\nimport math\nimport copy\nimport shutil\nfrom datetime import datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:13:34.462422Z","iopub.execute_input":"2025-07-07T08:13:34.462699Z","iopub.status.idle":"2025-07-07T08:13:34.466942Z","shell.execute_reply.started":"2025-07-07T08:13:34.462681Z","shell.execute_reply":"2025-07-07T08:13:34.466307Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class Actor(nn.Module):\n    def __init__(self, state_dim, action_dim, net_width):\n        super(Actor, self).__init__()\n        # L'input è in scala di grigi, quindi ha 1 canale\n        in_channels = 1\n\n        self.cnn_base = nn.Sequential(\n            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.ReLU()\n        )\n\n        # Calcola la dimensione dell'output della CNN in modo dinamico\n        with torch.no_grad():\n            # state_dim è (H, W), es: (84, 84)\n            dummy_input = torch.zeros(1, in_channels, *state_dim)\n            cnn_out_dim = self.cnn_base(dummy_input).view(1, -1).size(1)\n\n        self.fc1 = nn.Linear(cnn_out_dim, net_width)\n        self.fc_pi = nn.Linear(net_width, action_dim)\n\n    def forward(self, state):\n        # Gestisce sia un singolo stato (H, W) che un batch (B, H, W)\n        # Aggiunge la dimensione del canale per creare (B, C, H, W)\n        if len(state.shape) == 3:  # Batch di stati (B, H, W)\n            state = state.unsqueeze(1)\n        elif len(state.shape) == 2:  # Singolo stato (H, W)\n            state = state.unsqueeze(0).unsqueeze(0) # -> (1, 1, H, W)\n\n        x = self.cnn_base(state)\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        return self.fc_pi(x)\n\n    def pi(self, state, softmax_dim=1):\n        logits = self.forward(state)\n        probs = F.softmax(logits, dim=softmax_dim)\n        return probs\n\nclass Critic(nn.Module):\n    def __init__(self, state_dim, net_width):\n        super(Critic, self).__init__()\n        # L'input è in scala di grigi, quindi ha 1 canale\n        in_channels = 1\n\n        self.cnn_base = nn.Sequential(\n            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.ReLU()\n        )\n\n        # Calcola la dimensione dell'output della CNN in modo dinamico\n        with torch.no_grad():\n            # state_dim è (H, W), es: (84, 84)\n            dummy_input = torch.zeros(1, in_channels, *state_dim)\n            cnn_out_dim = self.cnn_base(dummy_input).view(1, -1).size(1)\n\n        self.fc1 = nn.Linear(cnn_out_dim, net_width)\n        self.fc_v = nn.Linear(net_width, 1)\n\n    def forward(self, state):\n        # Gestisce sia un singolo stato (H, W) che un batch (B, H, W)\n        # Aggiunge la dimensione del canale per creare (B, C, H, W)\n        if len(state.shape) == 3:  # Batch di stati (B, H, W)\n            state = state.unsqueeze(1)\n        elif len(state.shape) == 2:  # Singolo stato (H, W)\n            state = state.unsqueeze(0).unsqueeze(0) # -> (1, 1, H, W)\n\n        x = self.cnn_base(state)\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        return self.fc_v(x)\n\n\ndef evaluate_policy(env, agent, turns=3):\n    total_scores = 0\n    for i in range(turns):\n        s, info = env.reset()\n        done = False\n        episode_reward = 0\n        while not done:\n            a, logprob_a = agent.select_action(s, deterministic=True)\n            s_next, r, dw, tr, info = env.step(a)\n            done = dw or tr\n            episode_reward += r\n            s = s_next\n        total_scores += episode_reward\n    return total_scores / turns\n\ndef str2bool(v):\n    if isinstance(v, bool): return v\n    if v.lower() in ('yes', 'true', 't', 'y', '1'): return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'): return False\n    else: raise argparse.ArgumentTypeError('Boolean value expected.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:10:39.266101Z","iopub.execute_input":"2025-07-07T08:10:39.266467Z","iopub.status.idle":"2025-07-07T08:10:39.278797Z","shell.execute_reply.started":"2025-07-07T08:10:39.266447Z","shell.execute_reply":"2025-07-07T08:10:39.278012Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class PPO_discrete():\n    def __init__(self, **kwargs):\n        # Init hyperparameters for PPO agent, just like \"self.gamma = opt.gamma, self.lambd = opt.lambd, ...\"\n        self.__dict__.update(kwargs)\n        \n        # Set exploration parameters\n        self.initial_entropy_coef = self.initial_entropy_coef\n        self.min_entropy_coef = self.min_entropy_coef if hasattr(self, 'min_entropy_coef') else 0.01\n        self.initial_explore_steps = self.initial_explore_steps if hasattr(self, 'initial_explore_steps') else 10000\n        self.total_steps_taken = 0\n        self.entropy_coef = self.initial_entropy_coef\n\n        '''Build Actor and Critic'''\n        self.actor = Actor(self.state_dim, self.action_dim, self.net_width).to(self.dvc)\n        self.actor_optimizer = torch.optim.AdamW(self.actor.parameters(), lr=self.lr)\n        self.critic = Critic(self.state_dim, self.net_width).to(self.dvc)\n        self.critic_optimizer = torch.optim.AdamW(self.critic.parameters(), lr=self.lr)\n\n        '''Build Trajectory holder'''\n        self.s_hoder = np.zeros((self.T_horizon, *self.state_dim), dtype=np.float32)\n        self.a_hoder = np.zeros((self.T_horizon, 1), dtype=np.int64)\n        self.r_hoder = np.zeros((self.T_horizon, 1), dtype=np.float32)\n        self.s_next_hoder = np.zeros((self.T_horizon, *self.state_dim), dtype=np.float32)\n        self.logprob_a_hoder = np.zeros((self.T_horizon, 1), dtype=np.float32)\n        self.done_hoder = np.zeros((self.T_horizon, 1), dtype=np.bool_)\n        self.dw_hoder = np.zeros((self.T_horizon, 1), dtype=np.bool_)\n\n    def select_action(self, s, deterministic):\n        s = torch.from_numpy(s).float().unsqueeze(0).to(self.dvc)\n        with torch.no_grad():\n            pi = self.actor.pi(s, softmax_dim=0)\n            if deterministic:\n                a = torch.argmax(pi).item()\n                return a, None\n            else:\n                m = Categorical(pi)\n                a = m.sample().item()\n                pi_a = pi[0, a].item()\n                return a, pi_a\n\n    def train(self):\n        # Update total steps and manage entropy coefficient decay\n        self.total_steps_taken += self.T_horizon\n                \n        # Decay entropy coefficient but don't go below minimum\n        # at 100000 steps, entropy_coed should be 0.2\n        self.entropy_coef = self.min_entropy_coef + (self.initial_entropy_coef - self.min_entropy_coef) * \\\n                            math.exp(-1. * self.total_steps_taken / self.entropy_coef_decay)\n                            \n        # self.entropy_coef *= self.entropy_coef_decay INIZIALMENTE ERA COSI\n        if self.entropy_coef < self.min_entropy_coef:\n            self.entropy_coef = self.min_entropy_coef\n         \n        # DA RIMETTERE SE SI VUOLE MANTENERE ALTA ENTROPY COEF   \n        # For very early exploration, maintain high entropy coefficient\n        #if total_steps_taken < self.initial_explore_steps:\n        #    entropy_coef = self.initial_entropy_coef\n        \n        '''Prepare PyTorch data from Numpy data'''\n        s = torch.from_numpy(self.s_hoder).to(self.dvc)\n        a = torch.from_numpy(self.a_hoder).to(self.dvc)\n        r = torch.from_numpy(self.r_hoder).to(self.dvc)\n        s_next = torch.from_numpy(self.s_next_hoder).to(self.dvc)\n        old_prob_a = torch.from_numpy(self.logprob_a_hoder).to(self.dvc)\n        done = torch.from_numpy(self.done_hoder).to(self.dvc)\n        dw = torch.from_numpy(self.dw_hoder).to(self.dvc)\n\n        ''' Use TD+GAE+LongTrajectory to compute Advantage and TD target'''\n        with torch.no_grad():\n            vs = self.critic(s)\n            vs_ = self.critic(s_next)\n\n            '''dw(dead and win) for TD_target and Adv'''\n            deltas = r + self.gamma * vs_ * (~dw) - vs\n            deltas = deltas.cpu().flatten().numpy()\n            adv = [0]\n\n            '''done for GAE'''\n            for dlt, done in zip(deltas[::-1], done.cpu().flatten().numpy()[::-1]):\n                advantage = dlt + self.gamma * self.lambd * adv[-1] * (~done)\n                adv.append(advantage)\n            adv.reverse()\n            adv = copy.deepcopy(adv[0:-1])\n            adv = torch.tensor(adv).unsqueeze(1).float().to(self.dvc)\n            td_target = adv + vs\n            if self.adv_normalization:\n                adv = (adv - adv.mean()) / ((adv.std() + 1e-4))  #sometimes helps\n\n        \"\"\"PPO update\"\"\"\n        #Slice long trajectopy into short trajectory and perform mini-batch PPO update\n        optim_iter_num = int(math.ceil(s.shape[0] / self.batch_size))\n\n        for _ in range(self.K_epochs):\n            #Shuffle the trajectory, Good for training\n            perm = np.arange(s.shape[0])\n            np.random.shuffle(perm)\n            perm = torch.LongTensor(perm).to(self.dvc)\n            s, a, td_target, adv, old_prob_a = \\\n                s[perm].clone(), a[perm].clone(), td_target[perm].clone(), adv[perm].clone(), old_prob_a[perm].clone()\n\n            '''mini-batch PPO update'''\n            for i in range(optim_iter_num):\n                index = slice(i * self.batch_size, min((i + 1) * self.batch_size, s.shape[0]))\n\n                '''actor update'''\n                prob = self.actor.pi(s[index], softmax_dim=1)\n                entropy = Categorical(prob).entropy().sum(0, keepdim=True)\n                prob_a = prob.gather(1, a[index])\n                ratio = torch.exp(torch.log(prob_a) - torch.log(old_prob_a[index]))  # a/b == exp(log(a)-log(b))\n\n                surr1 = ratio * adv[index]\n                surr2 = torch.clamp(ratio, 1 - self.clip_rate, 1 + self.clip_rate) * adv[index]\n                a_loss = -torch.min(surr1, surr2) - self.entropy_coef * entropy\n\n                self.actor_optimizer.zero_grad()\n                a_loss.mean().backward()\n                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 40)\n                self.actor_optimizer.step()\n\n                '''critic update'''\n                c_loss = (self.critic(s[index]) - td_target[index]).pow(2).mean()\n                for name, param in self.critic.named_parameters():\n                    if 'weight' in name:\n                        c_loss += param.pow(2).sum() * self.l2_reg\n\n                self.critic_optimizer.zero_grad()\n                c_loss.backward()\n                self.critic_optimizer.step()\n\n    def put_data(self, s, a, r, s_next, logprob_a, done, dw, idx):\n        self.s_hoder[idx] = s\n        self.a_hoder[idx] = a\n        self.r_hoder[idx] = r\n        self.s_next_hoder[idx] = s_next\n        self.logprob_a_hoder[idx] = logprob_a\n        self.done_hoder[idx] = done\n        self.dw_hoder[idx] = dw\n\n    def save(self, episode, train_rewards, eval_rewards):\n        latest_path = \"./saved_models/ppo_model.pth\"\n        torch.save({\n            'episode': episode,\n            'model_state_dict': self.actor.state_dict(),\n            'critic_state_dict': self.critic.state_dict(),\n            'train_rewards': train_rewards,\n            'eval_rewards': eval_rewards,\n            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n            'device': str(self.dvc),\n            'total_steps_taken': self.total_steps_taken,\n        }, latest_path)\n\n    def load(self, latest_path):\n        checkpoint = torch.load(latest_path, map_location=self.dvc, weights_only=False)\n        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n        self.actor.load_state_dict(checkpoint['model_state_dict'])\n\n        return checkpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:10:43.587909Z","iopub.execute_input":"2025-07-07T08:10:43.588416Z","iopub.status.idle":"2025-07-07T08:10:43.607703Z","shell.execute_reply.started":"2025-07-07T08:10:43.588395Z","shell.execute_reply":"2025-07-07T08:10:43.607015Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Sostituzione di argparse per l'uso in un notebook\nclass Args:\n    def __init__(self):\n        self.dvc = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.EnvIdex = 0\n        self.write = True  # Abilita TensorBoard\n        self.render = False # Disabilita il rendering a schermo\n        self.Loadmodel = False\n        self.ModelIdex = 0\n        self.seed = 42\n        self.T_horizon = 2048\n        self.Max_train_steps = 5e5\n        self.save_interval = 10000\n        self.eval_interval = 2000\n        self.gamma = 0.99\n        self.lambd = 0.95\n        self.clip_rate = 0.2\n        self.K_epochs = 10\n        self.net_width = 512\n        self.lr = 2.5e-4\n        self.l2_reg = 0.0\n        self.batch_size = 256\n        self.initial_entropy_coef = 0.001\n        self.min_entropy_coef = 0.001\n        self.entropy_coef_decay = 200000\n        self.adv_normalization = True\n\nopt = Args()\nopt.dvc = torch.device(opt.dvc)\nprint(opt.__dict__)\n\ndef main():\n    EnvName = ['CarRacing-v3']\n    env = gym.make(EnvName[opt.EnvIdex], continuous=False, render_mode=\"rgb_array\")\n    opt.max_e_steps = env._max_episode_steps\n\n    env = GrayscaleObservation(env)\n    env = ResizeObservation(env, (84, 84))\n    opt.state_dim = env.observation_space.shape\n    opt.action_dim = env.action_space.n\n\n    eval_env = gym.make(EnvName[opt.EnvIdex], continuous=False)\n    eval_env = GrayscaleObservation(eval_env)\n    eval_env = ResizeObservation(eval_env, (84, 84))\n\n    torch.manual_seed(opt.seed)\n    np.random.seed(opt.seed)\n    print(f\"Env: {EnvName[opt.EnvIdex]}, StateDim: {opt.state_dim}, ActionDim: {opt.action_dim}, Seed: {opt.seed}\")\n\n    agent = PPO_discrete(**vars(opt))\n    train_rewards, eval_rewards = [], []\n\n    if opt.Loadmodel:\n        try:\n            checkpoint = agent.load(\"/kaggle/working/saved_models/ppo_model.pth\")\n            train_rewards = checkpoint.get('train_rewards', [])\n            eval_rewards = checkpoint.get('eval_rewards', [])\n            agent.total_steps_taken = checkpoint.get('total_steps_taken', 0)\n            print(f'Modello caricato dal passo {agent.total_steps_taken}...')\n        except FileNotFoundError:\n            print(\"Nessun modello salvato trovato. Inizio l'addestramento da zero.\")\n            opt.Loadmodel = False\n\n    traj_lenth, total_steps = 0, 0 if not opt.Loadmodel else agent.total_steps_taken\n    env_seed = opt.seed\n\n    while total_steps < opt.Max_train_steps:\n        s, info = env.reset(seed=env_seed)\n        env_seed += 1\n        done, episode_reward = False, 0\n\n        while not done:\n            a, logprob_a = agent.select_action(s, deterministic=False)\n            s_next, r, dw, tr, info = env.step(a)\n            done = dw or tr\n            episode_reward += r\n\n            if done: train_rewards.append((total_steps, episode_reward))\n\n            agent.put_data(s, a, r, s_next, logprob_a, done, dw, idx=traj_lenth)\n            s = s_next\n            traj_lenth += 1\n            total_steps += 1\n\n            if traj_lenth % opt.T_horizon == 0:\n                agent.train()\n                traj_lenth = 0\n\n            if total_steps % opt.eval_interval == 0:\n                score = evaluate_policy(eval_env, agent, turns=3)\n                eval_rewards.append(score)\n                print(f'Ep: {total_steps // opt.T_horizon}, TrainR: {train_rewards[-1][1]:.2f}, EvalR: {score:.2f}, Steps: {total_steps}, Entr: {agent.entropy_coef:.3f}')\n\n            if total_steps % opt.save_interval == 0:\n                print(f'Salvataggio modello al passo {total_steps}...')\n                agent.save(total_steps // opt.T_horizon, train_rewards, eval_rewards)\n\n    env.close()\n    eval_env.close()\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:18:32.211530Z","iopub.execute_input":"2025-07-07T09:18:32.212044Z","iopub.status.idle":"2025-07-07T09:19:20.422784Z","shell.execute_reply.started":"2025-07-07T09:18:32.212023Z","shell.execute_reply":"2025-07-07T09:19:20.421706Z"}},"outputs":[{"name":"stdout","text":"{'dvc': device(type='cuda'), 'EnvIdex': 0, 'write': True, 'render': False, 'Loadmodel': False, 'ModelIdex': 0, 'seed': 42, 'T_horizon': 2048, 'Max_train_steps': 500000.0, 'save_interval': 10000, 'eval_interval': 2000, 'gamma': 0.99, 'lambd': 0.95, 'clip_rate': 0.2, 'K_epochs': 10, 'net_width': 512, 'lr': 0.00025, 'l2_reg': 0.0, 'batch_size': 256, 'initial_entropy_coef': 0.001, 'min_entropy_coef': 0.001, 'entropy_coef_decay': 200000, 'adv_normalization': True}\nEnv: CarRacing-v3, StateDim: (84, 84), ActionDim: 5, Seed: 42\nEp: 0, TrainR: -63.70, EvalR: -93.31, Steps: 2000, Entr: 0.001\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1232274329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/1232274329.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtraj_lenth\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT_horizon\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mtraj_lenth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1512749563.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;34m'''actor update'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0mprob_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_prob_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# a/b == exp(log(a)-log(b))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m     72\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (256, 5)) of distribution Categorical(probs: torch.Size([256, 5])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        ...,\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<DivBackward0>)"],"ename":"ValueError","evalue":"Expected parameter probs (Tensor of shape (256, 5)) of distribution Categorical(probs: torch.Size([256, 5])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        ...,\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<DivBackward0>)","output_type":"error"}],"execution_count":19}]}